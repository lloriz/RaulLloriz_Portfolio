{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8947eeec-33b1-4e56-be6e-fcbcd7f8c014",
     "showTitle": true,
     "title": "Read Params"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import ast\n",
    "\n",
    "startDate = datetime.datetime.now()\n",
    "\n",
    "entity = str(getArgument(\"entity\")).replace(\"'\",\"\")\n",
    "print (\"Param -\\'entity': \" + str(entity))\n",
    "\n",
    "filesToProcessArg = str(getArgument(\"filesToProcess\"))\n",
    "filesToProcess = ast.literal_eval(filesToProcessArg)\n",
    "print (\"Param -\\'filesToProcess': \" + str(filesToProcess))\n",
    "\n",
    "executionMode = str(getArgument(\"executionMode\")).replace(\"'\",\"\")\n",
    "print (\"Param -\\'executionMode': \" + executionMode)\n",
    "\n",
    "etlDateString = str(getArgument(\"etlDate\"))\n",
    "etlDate = datetime.datetime.strptime(etlDateString, \"%Y-%m-%d  %H:%M:%S\")\n",
    "print (\"Param -\\'etlDate': \" + str(etlDate))\n",
    "\n",
    "dataSegment = 'pfu'\n",
    "dataSegmentFriendlyName = 'PFU'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read RawData Datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f04d94ea-20e3-4460-a1c6-c5b5d0c4a175",
     "showTitle": true,
     "title": "Read RawData Datalake"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    mainData = spark.read.format(\"avro\").load([file for file in filesToProcess if file.find(\"instance_BB\") >= 0][0] + \"*.avro\")\n",
    "    mainData = mainData.cache()\n",
    "except:\n",
    "    entityLog.exceptionMessage(\"Read raw datalake files\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange & format fields from sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a76dbde-902b-4888-9a1a-a3b6e6405992",
     "showTitle": true,
     "title": "Arrange & format fields from sources"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "try:\n",
    "    keyColumn = '{0}Id'.format(entity)\n",
    "\n",
    "    mappingMainData ={keyColumn:keyColumn,'ID':'snowflakeId','SOURCE_ID':'sourceId','NAME':'name','DESCRIPTION':'description','TYPE':'type','STAGE':'stage','ROW_INSERTED_TIME':'rowInsertedDatetime','ROW_UPDATED_TIME':'rowUpdatedDatetime','ROW_DELETED_TIME':'rowDeletedDatetime','downloadDate':'downloadDate',}\n",
    "\n",
    "    mainDataTransformed = mainData.withColumn(\"downloadDate\",lit(etlDate))\n",
    "    mainDataTransformed = mainDataTransformed.withColumn(keyColumn,col(\"ID\"))\n",
    "\n",
    "    columns2date = [\"ROW_INSERTED_TIME\",\"ROW_UPDATED_TIME\",\"ROW_DELETED_TIME\"]\n",
    "    for colname in columns2date:\n",
    "        mainDataTransformed = mainDataTransformed.withColumn(colname, to_timestamp(col(colname), 'yyyy-MM-dd HH:mm:ss.SSSSSSS'))\n",
    "\n",
    "\n",
    "    mainDataTransformed = mainDataTransformed.select([col(x).alias(y) for x,y in mappingMainData.items()])\n",
    "\n",
    "except:\n",
    "    entityLog.exceptionMessage(\"Arrange & format fields from sources\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest to Datalake Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3151fa9-a16a-47b6-b907-11713cb40a20",
     "showTitle": true,
     "title": "Ingest to Datalake"
    }
   },
   "outputs": [],
   "source": [
    "from necsiaadbutils import *\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    partitionConfig = {\"partitionFormat\":\"repartition\",\"partitionValue\":20}\n",
    "    entityDLT = DatalakeTransformation(dataSegment,dataSegmentFriendlyName,entity,etlDate,mainDataTransformed,loadType=executionMode)\n",
    "    entityDLT.runFullTransformation(partitionConfig)\n",
    "    print(\"Elapsed time: {0}\".format(datetime.now() - startDate))\n",
    "except:\n",
    "    entityLog.exceptionMessage(\"Ingest to Datalake\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TransformDataInstance_BB",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
